{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 5\n",
    "%autosave 15\n",
    "\n",
    "from __future__ import division\n",
    "import operator\n",
    "import functools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import base64\n",
    "import csv\n",
    "import gzip\n",
    "import zlib\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRACE_NUM = 1000\n",
    "import logging\n",
    "from imp import reload\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "\n",
    "def trace(items_num, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d\" % items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_utf8(text):\n",
    "    if isinstance(text, str): text = text.encode('utf8', \"ignore\")\n",
    "    return text\n",
    "\n",
    "def convert2unicode(f):\n",
    "    def tmp(text):\n",
    "        if not isinstance(text, str): text = text.decode('utf8', \"ignore\")\n",
    "        return f(text)\n",
    "    return tmp\n",
    "\n",
    "def convert2lower(f):\n",
    "    def tmp(text):        \n",
    "        return f(text.lower())\n",
    "    return tmp\n",
    "\n",
    "#P.S. Декораторы могут усложнять отладку, так что от них вполне можно отказаться и воспользоваться copy-paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "\n",
    "class TextHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self._text = []\n",
    "        self._title = []\n",
    "        self._in_title = False\n",
    "        self._in_a = False\n",
    "        self._a = []\n",
    "\n",
    "    def handle_data(self, data): \n",
    "        text = data.strip()        \n",
    "        if len(text) > 0:\n",
    "            text = re.sub('[ \\t\\r\\n]+', ' ', text)\n",
    "            if (self._in_title == True):\n",
    "                self._title.append(text + ' ')\n",
    "                self._in_title = False\n",
    "            else:\n",
    "                self._text.append(text + ' ')\n",
    "                if (self._in_a == True):\n",
    "                    self._a.append(text + ' ')\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):        \n",
    "        if tag == 'p':\n",
    "            self._text.append('\\n\\n')\n",
    "        elif tag == 'br':\n",
    "            self._text.append('\\n')\n",
    "        elif tag == 'title':\n",
    "            self._in_title = True\n",
    "        elif tag == 'a':\n",
    "            self._in_a = True\n",
    "\n",
    "    def handle_startendtag(self, tag, attrs):        \n",
    "        if tag == 'br':\n",
    "            self._text.append('\\n\\n')\n",
    "        elif tag == 'title':\n",
    "            self._in_title = False\n",
    "            print('/TITLE')\n",
    "        elif tag == 'a':\n",
    "            self._in_a = False\n",
    "\n",
    "    def text(self):\n",
    "        return (''.join(self._title).strip(), ''.join(self._text).strip(), ''.join(self._a).strip())\n",
    "\n",
    "@convert2unicode\n",
    "def html2text_parser(text):\n",
    "    parser = TextHTMLParser()\n",
    "    parser.feed(text)\n",
    "    return parser.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@convert2lower\n",
    "@convert2unicode\n",
    "def easy_tokenizer(text):\n",
    "    word = str()\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = str()\n",
    "    if word: yield word\n",
    "\n",
    "PYMORPHY_CACHE = {}\n",
    "MORPH = None\n",
    "#hint, чтобы установка pymorphy2 не была бы обязательной\n",
    "def get_lemmatizer():\n",
    "    import pymorphy2\n",
    "    global MORPH\n",
    "    if MORPH is None: MORPH = pymorphy2.MorphAnalyzer()\n",
    "    return MORPH\n",
    "\n",
    "@convert2lower\n",
    "@convert2unicode\n",
    "def pymorphy_tokenizer(text):\n",
    "    global PYMORPHY_CACHE\n",
    "    for word in easy_tokenizer(text):\n",
    "        word_hash = hash(word)\n",
    "        if word_hash not in PYMORPHY_CACHE:\n",
    "            PYMORPHY_CACHE[word_hash] = get_lemmatizer().parse(word)[0].normal_form            \n",
    "        yield PYMORPHY_CACHE[word_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return [title, text, text in <a>]\n",
    "def html2word(raw_html, to_text=html2text_parser, tokenizer=easy_tokenizer):\n",
    "    tt = to_text(raw_html)\n",
    "    doc = []\n",
    "    for t in tt:\n",
    "        doc.append(list(tokenizer(t.lower())))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'features'])\n",
    "SPAM = 1\n",
    "LEGIT = 0\n",
    "TEST_COUNT = 0\n",
    "def load_csv(input_file_name):\n",
    "    global TEST_COUNT\n",
    "    TEST_COUNT = 0\n",
    "    frequencies = defaultdict(lambda:0)\n",
    "    classes = defaultdict(lambda:0)\n",
    "    counts = defaultdict(lambda:0)\n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name) as input_file:            \n",
    "        headers = input_file.readline()        \n",
    "        for i, line in enumerate(input_file):            \n",
    "            trace(i)\n",
    "            parts = line.decode('utf-8').strip().split('\\t')\n",
    "            url_id = int(parts[0])\n",
    "            mark = int(parts[1])\n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)            \n",
    "            title, data, aa = html2word(html_data)     \n",
    "            add_data(mark, data, frequencies, classes, counts)\n",
    "#             features = calc_features_f(url, (title, data, aa, len(line)))            \n",
    "#             yield DocItem(url_id, mark, url, features)\n",
    "            TEST_COUNT += 1\n",
    "    return frequencies, classes, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1\tКоличество слов на странице\n",
    "# 2\tСредняя длина слова\n",
    "# 3\tКоличество слов в заголовке страниц (слова в теге <html><head><title> Some text </title>)\n",
    "# 4\tКоличество слов в анкорах ссылок (<html><body><a> Some text </a>)\n",
    "# 5\tКоэффициент сжатия\n",
    "\n",
    "def calc_features(url, data):\n",
    "    title, words, aa, cnt_letters = data\n",
    "    words_num = len(words)\n",
    "    ss = sum(len(w) for w in words)\n",
    "    avg_word_len = ss // words_num    \n",
    "    title_words_num = len(title)\n",
    "    anchor_words_num = len(aa)\n",
    "    compression_level = float(cnt_letters) / ss\n",
    "    \n",
    "    return [words_num, avg_word_len, title_words_num, anchor_words_num, compression_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_data(clazz, data, frequencies, classes, counts):\n",
    "    for word in data:\n",
    "        frequencies[clazz, word] += 1.0\n",
    "        counts[word] += 1.0\n",
    "    classes[clazz] += len(data)\n",
    "    \n",
    "def training(frequencies, classes, counts):\n",
    "    for clazz, word in frequencies:\n",
    "        frequencies[clazz, word] = (frequencies[clazz, word] + 1) / (counts[word] + 1)\n",
    "        # можно размытие по лапласу, как будто каждо слово видели на 1 раз больше\n",
    "#         frequencies[clazz, word] = (frequencies[clazz, word] + 1.0) / (1.0 + counts[word])\n",
    "    for clazz in classes:\n",
    "        classes[clazz] /= TEST_COUNT\n",
    "    return classes, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errorWeight = defaultdict(lambda:1)\n",
    "errorWeight[SPAM] = 1\n",
    "errorWeight[LEGIT] = 1\n",
    "def classify(classifier, data):\n",
    "    classes, frequencies = classifier\n",
    "    #P(C|o_1..o_n) = P(c)*prod(P(o_i|c)\n",
    "    # result class = argmax(c, P(c)*prod(P(o_i|c))) = argmin(c, -log(P(c) - sum(log(P(o_i|c)))))\n",
    "    spamProb = -log(classes[SPAM])\n",
    "    legitProb = -log(classes[LEGIT])\n",
    "    for word in data:\n",
    "        spamProb -= log(frequencies[SPAM, word] + 10 ** (-7))\n",
    "        legitProb -= log(frequencies[LEGIT, word] + 10 ** (-7))\n",
    "    spamProb -= log(errorWeight[SPAM])\n",
    "    legitProb -= log(errorWeight[LEGIT])\n",
    "    if (abs(spamProb - legitProb) < 20):\n",
    "        return LEGIT\n",
    "    if (spamProb < legitProb):\n",
    "        return SPAM\n",
    "    return LEGIT\n",
    "#     return min(classes.keys(), key = lambda clazz: -log(classes[clazz]) -\n",
    "#                sum(log(frequencies[clazz, word] + 10 ** (-7)) for word in file[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(input_file_name, curClassifier):\n",
    "    predict = []\n",
    "    answer = []\n",
    "    counts = [[0, 0], [0, 0]]\n",
    "    cnt = 0    \n",
    "\n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name) as input_file:            \n",
    "        headers = input_file.readline()        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.decode('utf-8').strip().split('\\t')\n",
    "            url_id = int(parts[0])\n",
    "            clazz = bool(int(parts[1]))\n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)            \n",
    "            title, data, aa = html2word(html_data)                        \n",
    "            predClass = classify(curClassifier, data)\n",
    "            predict.append(int(predClass == SPAM))\n",
    "            answer.append(int(clazz == SPAM))\n",
    "            counts[predClass == SPAM][clazz == SPAM] += 1\n",
    "            cnt += 1\n",
    "    return (counts, predict, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:37:08 INFO:Complete items 00000\n",
      "01:37:33 INFO:Complete items 01000\n",
      "01:37:58 INFO:Complete items 02000\n",
      "01:38:20 INFO:Complete items 03000\n",
      "01:38:42 INFO:Complete items 04000\n",
      "01:39:06 INFO:Complete items 05000\n",
      "01:39:26 INFO:Complete items 06000\n",
      "01:39:50 INFO:Complete items 07000\n",
      "01:39:52 INFO:Complete items 00000\n",
      "01:40:19 INFO:Complete items 01000\n",
      "01:40:41 INFO:Complete items 02000\n",
      "01:41:03 INFO:Complete items 03000\n",
      "01:41:26 INFO:Complete items 04000\n",
      "01:41:50 INFO:Complete items 05000\n",
      "01:42:11 INFO:Complete items 06000\n",
      "01:42:36 INFO:Complete items 07000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error =  0.0170357751278\n",
      "f1 score =  0.985188842261\n",
      "test count =  7044\n",
      "SPAM is SPAM =  2933\n",
      "SPAM is LEGIT =  118\n",
      "LEGIT is SPAM =  2\n",
      "LEGIT is LEGIT =  3991\n"
     ]
    }
   ],
   "source": [
    "# folds = [getFiles(i) for i in range(1, 11)]\n",
    "# meanClassifier = defaultdict(lambda:0), defaultdict(lambda:0)\n",
    "# for trains in folds:\n",
    "#     curClassifier = training(trains)\n",
    "#     meanClassifier = getMeanClassifier(curClassifier, meanClassifier)\n",
    "\n",
    "TRAIN_DATA_FILE  = 'kaggle_train_data_tab.csv.gz'\n",
    "# train_docs = list(load_csv(TRAIN_DATA_FILE))\n",
    "frequencies, classes, counts = load_csv(TRAIN_DATA_FILE)\n",
    "curClassifier = training(frequencies, classes, counts)\n",
    "# kf = KFold(n_splits=2)\n",
    "# for train, test in kf.split(X):\n",
    "counts, predict, answer = get_prediction(TRAIN_DATA_FILE, curClassifier)\n",
    "print(\"mean squared error = \", mean_squared_error(predict, answer))\n",
    "print(\"f1 score = \", f1_score(predict, answer))\n",
    "print(\"test count = \", TEST_COUNT)\n",
    "print(\"SPAM is SPAM = \", counts[0][0])\n",
    "print(\"SPAM is LEGIT = \", counts[0][1])\n",
    "print(\"LEGIT is SPAM = \", counts[1][0])\n",
    "print(\"LEGIT is LEGIT = \", counts[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMeanClassifier(classifier1, classifier2):\n",
    "    curClasses, curFrequencies = classifier1\n",
    "    meanClasses, meanFrequencies = classifier2\n",
    "    for clazz in curClasses:\n",
    "        meanClasses[clazz] = (meanClasses[clazz] + curClasses[clazz]) / 2\n",
    "    for clazz, word in curFrequencies:\n",
    "        meanFrequencies[clazz, word] = (meanFrequencies[clazz, word] + curFrequencies[clazz, word]) / 2\n",
    "    return meanClasses, meanFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:46:54 INFO:Complete items 00000\n",
      "01:47:28 INFO:Complete items 01000\n",
      "01:47:56 INFO:Complete items 02000\n",
      "01:48:24 INFO:Complete items 03000\n",
      "01:48:49 INFO:Complete items 04000\n",
      "01:49:13 INFO:Complete items 05000\n",
      "01:49:41 INFO:Complete items 06000\n",
      "01:50:06 INFO:Complete items 07000\n",
      "01:50:33 INFO:Complete items 08000\n",
      "01:51:00 INFO:Complete items 09000\n",
      "01:51:56 INFO:Complete items 10000\n",
      "01:52:27 INFO:Complete items 11000\n",
      "01:52:57 INFO:Complete items 12000\n",
      "01:53:23 INFO:Complete items 13000\n",
      "01:53:55 INFO:Complete items 14000\n",
      "01:54:26 INFO:Complete items 15000\n",
      "01:54:59 INFO:Complete items 16000\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_FILE  = 'kaggle_test_data_tab.csv.gz'\n",
    "\n",
    "with open('my_submission.csv' , 'w') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['Id','Prediction'])\n",
    "    with gzip.open(TEST_DATA_FILE) if TEST_DATA_FILE.endswith('gz') else open(TEST_DATA_FILE) as input_file:            \n",
    "        headers = input_file.readline()        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.decode('utf-8').strip().split('\\t')\n",
    "            url_id = int(parts[0])        \n",
    "    #         url = parts[2]        \n",
    "            html_data = base64.b64decode(parts[3])\n",
    "            title, data, aa = html2word(html_data)\n",
    "            writer.writerow([url_id, classify(curClassifier, data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
