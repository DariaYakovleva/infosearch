{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 5\n",
    "%autosave 15\n",
    "\n",
    "from __future__ import division\n",
    "import operator\n",
    "import functools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import base64\n",
    "import csv\n",
    "import gzip\n",
    "import zlib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRACE_NUM = 1000\n",
    "import logging\n",
    "from imp import reload\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "\n",
    "def trace(items_num, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d\" % items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_utf8(text):\n",
    "    if isinstance(text, str): text = text.encode('utf8', \"ignore\")\n",
    "    return text\n",
    "\n",
    "def convert2unicode(f):\n",
    "    def tmp(text):\n",
    "        if not isinstance(text, str): text = text.decode('utf8', \"ignore\")\n",
    "        return f(text)\n",
    "    return tmp\n",
    "\n",
    "def convert2lower(f):\n",
    "    def tmp(text):        \n",
    "        return f(text.lower())\n",
    "    return tmp\n",
    "\n",
    "#P.S. Декораторы могут усложнять отладку, так что от них вполне можно отказаться и воспользоваться copy-paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "\n",
    "class TextHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self._text = []\n",
    "        self._title = []\n",
    "        self._in_title = False        \n",
    "\n",
    "    def handle_data(self, data): \n",
    "        text = data.strip()        \n",
    "        if len(text) > 0:\n",
    "            text = re.sub('[ \\t\\r\\n]+', ' ', text)\n",
    "            if (self._in_title == True):\n",
    "                self._title.append(text + ' ')\n",
    "                self._in_title = False\n",
    "            else:\n",
    "                self._text.append(text + ' ')                \n",
    "\n",
    "    def handle_starttag(self, tag, attrs):        \n",
    "        if tag == 'p':\n",
    "            self._text.append('\\n\\n')\n",
    "        elif tag == 'br':\n",
    "            self._text.append('\\n')\n",
    "        elif tag == 'title':\n",
    "            self._in_title = True        \n",
    "\n",
    "    def handle_startendtag(self, tag, attrs):        \n",
    "        if tag == 'br':\n",
    "            self._text.append('\\n\\n')\n",
    "        elif tag == 'title':\n",
    "            self._in_title = False        \n",
    "\n",
    "    def text(self):\n",
    "        return (''.join(self._title).strip(), ''.join(self._text).strip())\n",
    "\n",
    "@convert2unicode\n",
    "def html2text_parser(text):\n",
    "    parser = TextHTMLParser()\n",
    "    parser.feed(text)\n",
    "    return parser.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@convert2lower\n",
    "@convert2unicode\n",
    "def easy_tokenizer(text):\n",
    "    word = str()\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = str()\n",
    "    if word: yield word\n",
    "\n",
    "PYMORPHY_CACHE = {}\n",
    "MORPH = None\n",
    "#hint, чтобы установка pymorphy2 не была бы обязательной\n",
    "def get_lemmatizer():\n",
    "    import pymorphy2\n",
    "    global MORPH\n",
    "    if MORPH is None: MORPH = pymorphy2.MorphAnalyzer()\n",
    "    return MORPH\n",
    "\n",
    "@convert2lower\n",
    "@convert2unicode\n",
    "def pymorphy_tokenizer(text):\n",
    "    global PYMORPHY_CACHE\n",
    "    for word in easy_tokenizer(text):\n",
    "        word_hash = hash(word)\n",
    "        if word_hash not in PYMORPHY_CACHE:\n",
    "            PYMORPHY_CACHE[word_hash] = get_lemmatizer().parse(word)[0].normal_form            \n",
    "        yield PYMORPHY_CACHE[word_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return [title, text, text in <a>]\n",
    "def html2word(raw_html, to_text=html2text_parser, tokenizer=easy_tokenizer):\n",
    "    title, data = to_text(raw_html)\n",
    "    return (list(tokenizer(title.lower())), list(tokenizer(data.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'features'])\n",
    "\n",
    "def load_csv(input_file_name):    \n",
    "    train = []\n",
    "    ans = []\n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name) as input_file:            \n",
    "        headers = input_file.readline()        \n",
    "        for i, line in enumerate(input_file):            \n",
    "            trace(i)\n",
    "            parts = line.decode('utf-8').strip().split('\\t')\n",
    "            url_id = int(parts[0])\n",
    "            mark = int(parts[1])\n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)            \n",
    "            title, data = html2word(html_data)\n",
    "            train.append(' '.join(data))\n",
    "            ans.append(mark)\n",
    "    return (train, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:01:06 INFO:Complete items 00000\n",
      "23:01:26 INFO:Complete items 01000\n",
      "23:01:44 INFO:Complete items 02000\n",
      "23:02:02 INFO:Complete items 03000\n",
      "23:02:18 INFO:Complete items 04000\n",
      "23:02:36 INFO:Complete items 05000\n",
      "23:02:53 INFO:Complete items 06000\n",
      "23:03:14 INFO:Complete items 07000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_FILE  = 'kaggle_train_data_tab.csv.gz'\n",
    "train, ans = load_csv(TRAIN_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score =  0.9950799508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "text_clf.fit(train, ans)\n",
    "predicted = text_clf.predict(train)\n",
    "print('f1 score = ', f1_score(ans, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ok\n",
      "predict ok\n",
      "f1 score =  0.979762061548\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "text_clf2 = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf2.fit(train, ans)\n",
    "print('fit ok')\n",
    "predicted = text_clf2.predict(train)\n",
    "print('predict ok')\n",
    "print('f1 score = ', f1_score(ans, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ok\n",
      "predict ok\n",
      "f1 score =  0.999034579603\n"
     ]
    }
   ],
   "source": [
    "text_clf3 = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', RandomForestClassifier())])\n",
    "\n",
    "text_clf3.fit(train, ans)\n",
    "print('fit ok')\n",
    "predicted = text_clf3.predict(train)\n",
    "print('predict ok')\n",
    "print('f1 score = ', f1_score(ans, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7044\n",
      "20705\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "for x in train2:\n",
    "    train.append(' '.join(x))\n",
    "print(len(train))\n",
    "ans = ans + ans2\n",
    "# train2 = []\n",
    "# ans2 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:31:55 INFO:Complete items 00000\n",
      "23:34:10 INFO:Complete items 01000\n",
      "23:36:49 INFO:Complete items 02000\n",
      "23:38:41 INFO:Complete items 03000\n",
      "23:40:32 INFO:Complete items 04000\n",
      "23:42:24 INFO:Complete items 05000\n",
      "23:44:19 INFO:Complete items 06000\n",
      "23:46:09 INFO:Complete items 07000\n",
      "23:48:03 INFO:Complete items 08000\n",
      "23:49:54 INFO:Complete items 09000\n",
      "23:51:55 INFO:Complete items 10000\n",
      "23:53:44 INFO:Complete items 11000\n",
      "23:55:33 INFO:Complete items 12000\n",
      "23:57:21 INFO:Complete items 13000\n",
      "23:59:13 INFO:Complete items 14000\n",
      "00:01:15 INFO:Complete items 15000\n",
      "00:03:19 INFO:Complete items 16000\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_FILE  = 'kaggle_test_data_tab.csv.gz'\n",
    "\n",
    "train2 = []\n",
    "ans2 = []\n",
    "fout2 = open('my_submission2.csv' , 'w')\n",
    "fout3 = open('my_submission3.csv' , 'w')\n",
    "with open('my_submission.csv' , 'w') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['Id','Prediction'])\n",
    "    writer2 = csv.writer(fout2)\n",
    "    writer2.writerow(['Id','Prediction'])\n",
    "\n",
    "    writer3 = csv.writer(fout3)\n",
    "    writer3.writerow(['Id','Prediction'])\n",
    "\n",
    "    with gzip.open(TEST_DATA_FILE) if TEST_DATA_FILE.endswith('gz') else open(TEST_DATA_FILE) as input_file:            \n",
    "        headers = input_file.readline()        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.decode('utf-8').strip().split('\\t')\n",
    "            url_id = int(parts[0])        \n",
    "            title, data = html2word(base64.b64decode(parts[3]))   \n",
    "            ss = [' '.join(data)]\n",
    "            predicted = [text_clf.predict(ss)[0], text_clf2.predict(ss)[0], text_clf3.predict(ss)[0]]\n",
    "            \n",
    "            writer2.writerow([url_id, predicted[1]])\n",
    "            writer3.writerow([url_id, predicted[2]])\n",
    "            predicted.sort()\n",
    "            writer.writerow([url_id, predicted[1]])\n",
    "            cnt = sum(predicted)\n",
    "            if cnt == 0:\n",
    "                train2.append(ss)\n",
    "                ans2.append(0)\n",
    "            elif cnt == 3:\n",
    "                train2.append(ss)\n",
    "                ans2.append(1)                        \n",
    "fout2.close()\n",
    "fout3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
